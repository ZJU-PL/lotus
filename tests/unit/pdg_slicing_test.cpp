/**
 * @file pdg_slicing_test.cpp
 * @brief Comprehensive tests for PDG slicing primitives using real benchmark files
 *
 * This file tests the actual lib/IR/PDG/Slicing.cpp implementation using
 * real LLVM bitcode files from the benchmarks directory.
 */

#include <gtest/gtest.h>
#include <memory>
#include <string>
#include <vector>
#include <set>
#include <cstdlib>
#include <sys/stat.h>
#include <unistd.h>

#include "IR/PDG/Slicing.h"
#include "IR/PDG/ContextSensitiveSlicing.h"
#include "IR/PDG/PDGCallGraph.h"

#include "llvm/IR/LLVMContext.h"
#include "llvm/IR/Module.h"
#include "llvm/IRReader/IRReader.h"
#include "llvm/Support/SourceMgr.h"
#include "llvm/Support/CommandLine.h"

using namespace llvm;
using namespace pdg;

// Command line options for test configuration
static cl::opt<std::string> BenchmarkDir("benchmark-dir",
                                        cl::desc("Directory containing benchmark files"),
                                        cl::init(""));

static cl::opt<std::string> TestFile("test-file",
                                    cl::desc("Specific benchmark file to test"),
                                    cl::init(""));

/**
 * @brief Test fixture for PDG slicing primitives
 * 
 * This test class provides a common setup for testing PDG slicing functionality
 * using real LLVM bitcode files from the benchmarks directory. It handles
 * loading LLVM modules, building PDGs, and collecting test nodes for slicing.
 */
class PDGSlicingTest : public ::testing::Test {
protected:
  /**
   * @brief Set up test environment before each test
   * 
   * This method initializes the test environment by:
   * 1. Creating an LLVM context
   * 2. Finding and loading a benchmark file
   * 3. Building the PDG from the loaded module
   * 4. Collecting test nodes for slicing operations
   */
  void SetUp() override {
    Context = std::make_unique<LLVMContext>();
    
    // Find benchmark directory dynamically
    std::vector<std::string> searchPaths = {
      BenchmarkDir,
      "../benchmarks/spec2006",
      "benchmarks/spec2006",
      "../benchmarks/spec2006"
    };
    
    std::string benchmarkDir;
    for (const auto& path : searchPaths) {
      struct stat st;
      if (stat(path.c_str(), &st) == 0 && S_ISDIR(st.st_mode)) {
        benchmarkDir = path;
        break;
      }
    }
    
    if (benchmarkDir.empty()) {
      GTEST_SKIP() << "Benchmark directory not found";
      return;
    }
    
    // Try to load a benchmark file
    std::vector<std::string> files = !TestFile.empty() ? 
      std::vector<std::string>{TestFile} : 
      std::vector<std::string>{"998.specrand.bc", "999.specrand.bc"};
    
    SMDiagnostic Err;
    for (const auto& filename : files) {
      std::string fullPath = benchmarkDir;
      fullPath += "/";
      fullPath += filename;
      struct stat st;
      if (stat(fullPath.c_str(), &st) == 0) {
        Module = parseIRFile(fullPath, Err, *Context);
        if (Module) break;
      }
    }
    
    if (!Module) {
      GTEST_SKIP() << "Could not load benchmark file from " << benchmarkDir;
      return;
    }
    
    try {
      // Reset singleton state to avoid accumulation between tests
      ProgramGraph &PDG = ProgramGraph::getInstance();
      PDG.reset(); // Clear all state
      PDG.build(*Module);
      PDG.bindDITypeToNodes(*Module);
      this->PDG = &PDG;
      
      // Reset and rebuild call graph singleton
      PDGCallGraph &callGraph = PDGCallGraph::getInstance();
      callGraph.reset(); // Clear all state
      callGraph.build(*Module);
      CallGraph = &callGraph; // Store reference to singleton
      
      collectTestNodes();
    } catch (...) {
      GTEST_SKIP() << "Exception during PDG setup";
    }
  }
  
  void TearDown() override {
    Module.reset();
    Context.reset();
    CallGraph = nullptr;
  }
  
  void collectTestNodes() {
    testNodes.clear();
    if (!Module) return;
    
    for (auto &F : *Module) {
      if (F.isDeclaration() || F.empty()) continue;
      
      // Add function entry node
      if (PDG->hasFuncWrapper(F)) {
        if (auto *entryNode = PDG->getFuncWrapper(F)->getEntryNode()) {
          testNodes.push_back(entryNode);
        }
      }
      
      // Add first 3 instruction nodes per function
      int instCount = 0;
      for (auto &BB : F) {
        for (auto &I : BB) {
          if (instCount >= 3) break;
          if (auto *node = PDG->getNode(I)) {
            testNodes.push_back(node);
            instCount++;
          }
        }
        if (instCount >= 3) break;
      }
    }
  }
  
  // Helper to validate slice contains start node and has reasonable size
  void validateSlice(const std::set<Node*> &slice, Node *startNode, const std::string &sliceType) {
    EXPECT_GT(slice.size(), 0) << sliceType << " should not be empty";
    EXPECT_TRUE(slice.find(startNode) != slice.end()) << "Start node should be in its own " << sliceType;
    auto stats = SlicingUtils::getSliceStatistics(slice);
    EXPECT_GT(stats["total_nodes"], 0) << sliceType << " should contain nodes";
  }
  
  // Helper to check if we have enough test nodes
  bool hasMinimumNodes(size_t required) {
    return PDG && testNodes.size() >= required;
  }
  
  std::unique_ptr<LLVMContext> Context;
  std::unique_ptr<Module> Module;
  PDGCallGraph *CallGraph = nullptr;
  ProgramGraph *PDG = nullptr;
  std::vector<Node*> testNodes;
};

TEST_F(PDGSlicingTest, ForwardSlicingBasic) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ForwardSlicing forwardSlicer(*PDG);
  auto slice = forwardSlicer.computeSlice(*testNodes[0]);
  validateSlice(slice, testNodes[0], "Forward slice");
}

TEST_F(PDGSlicingTest, BackwardSlicingBasic) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  BackwardSlicing backwardSlicer(*PDG);
  auto slice = backwardSlicer.computeSlice(*testNodes[0]);
  validateSlice(slice, testNodes[0], "Backward slice");
}

TEST_F(PDGSlicingTest, ProgramChoppingBasic) {
  if (!hasMinimumNodes(2)) {
    GTEST_SKIP() << "Need at least 2 test nodes for chopping";
    return;
  }
  
  ProgramChopping chopper(*PDG);
  auto chop = chopper.computeChop(*testNodes[0], *testNodes[1]);
  
  EXPECT_GE(chop.size(), 0) << "Chop should be non-negative size";
  auto stats = SlicingUtils::getSliceStatistics(chop);
  EXPECT_GE(stats["total_nodes"], 0) << "Chop should have valid statistics";
}

TEST_F(PDGSlicingTest, EdgeTypeFiltering) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ForwardSlicing forwardSlicer(*PDG);
  Node *startNode = testNodes[0];
  
  auto dataSlice = forwardSlicer.computeSlice(*startNode, SlicingUtils::getDataDependencyEdges());
  auto controlSlice = forwardSlicer.computeSlice(*startNode, SlicingUtils::getControlDependencyEdges());
  auto paramSlice = forwardSlicer.computeSlice(*startNode, SlicingUtils::getParameterDependencyEdges());
  
  EXPECT_TRUE(dataSlice.find(startNode) != dataSlice.end());
  EXPECT_TRUE(controlSlice.find(startNode) != controlSlice.end());
  EXPECT_TRUE(paramSlice.find(startNode) != paramSlice.end());
}

TEST_F(PDGSlicingTest, DepthLimitedSlicing) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ForwardSlicing forwardSlicer(*PDG);
  Node *startNode = testNodes[0];
  
  auto slice1 = forwardSlicer.computeSliceWithDepth(*startNode, 1);
  auto slice2 = forwardSlicer.computeSliceWithDepth(*startNode, 2);
  auto sliceUnlimited = forwardSlicer.computeSlice(*startNode);
  
  EXPECT_LE(slice1.size(), slice2.size()) << "Depth 1 slice should be smaller than depth 2";
  EXPECT_LE(slice2.size(), sliceUnlimited.size()) << "Depth 2 slice should be smaller than unlimited";
  
  EXPECT_TRUE(slice1.find(startNode) != slice1.end());
  EXPECT_TRUE(slice2.find(startNode) != slice2.end());
  EXPECT_TRUE(sliceUnlimited.find(startNode) != sliceUnlimited.end());
}

TEST_F(PDGSlicingTest, MultipleStartNodes) {
  if (!hasMinimumNodes(3)) {
    GTEST_SKIP() << "Need at least 3 test nodes for multiple start nodes test";
    return;
  }
  
  ForwardSlicing::NodeSet startNodes;
  for (size_t i = 0; i < std::min(size_t(3), testNodes.size()); ++i) {
    startNodes.insert(testNodes[i]);
  }
  
  ForwardSlicing forwardSlicer(*PDG);
  auto slice = forwardSlicer.computeSlice(startNodes);
  
  for (auto *node : startNodes) {
    EXPECT_TRUE(slice.find(node) != slice.end()) << "All start nodes should be in the slice";
  }
  
  auto stats = SlicingUtils::getSliceStatistics(slice);
  EXPECT_GE(stats["total_nodes"], startNodes.size()) << "Slice should contain at least the start nodes";
}

TEST_F(PDGSlicingTest, PathFinding) {
  if (!hasMinimumNodes(2)) {
    GTEST_SKIP() << "Need at least 2 test nodes for path finding test";
    return;
  }
  
  ProgramChopping chopper(*PDG);
  bool hasPath = chopper.hasPath(*testNodes[0], *testNodes[1]);
  
  if (hasPath) {
    try {
      auto paths = chopper.findAllPaths(*testNodes[0], *testNodes[1], 2);
      
      for (const auto &path : paths) {
        if (!path.empty()) {
          EXPECT_EQ(path[0], testNodes[0]) << "Path should start with source node";
          EXPECT_EQ(path.back(), testNodes[1]) << "Path should end with sink node";
        }
      }
    } catch (...) {
      // Path finding may fail due to complexity - don't fail the test
    }
  }
}

TEST_F(PDGSlicingTest, SliceStatistics) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ForwardSlicing forwardSlicer(*PDG);
  auto slice = forwardSlicer.computeSlice(*testNodes[0]);
  auto stats = SlicingUtils::getSliceStatistics(slice);
  
  EXPECT_TRUE(stats.find("total_nodes") != stats.end()) << "Should have total_nodes statistic";
  EXPECT_GT(stats["total_nodes"], 0) << "Total nodes should be positive";
  
  bool hasNodeTypeStats = false;
  for (const auto &pair : stats) {
    if (pair.first.find("node_type_") == 0) {
      hasNodeTypeStats = true;
      break;
    }
  }
  EXPECT_TRUE(hasNodeTypeStats) << "Should have node type statistics";
}

TEST_F(PDGSlicingTest, EmptySliceHandling) {
  if (!PDG) {
    GTEST_SKIP() << "No PDG available";
    return;
  }
  
  ForwardSlicing forwardSlicer(*PDG);
  BackwardSlicing backwardSlicer(*PDG);
  
  ForwardSlicing::NodeSet emptyStartNodes;
  BackwardSlicing::NodeSet emptyEndNodes;
  
  auto forwardSlice = forwardSlicer.computeSlice(emptyStartNodes);
  auto backwardSlice = backwardSlicer.computeSlice(emptyEndNodes);
  
  EXPECT_EQ(forwardSlice.size(), 0) << "Empty start nodes should produce empty slice";
  EXPECT_EQ(backwardSlice.size(), 0) << "Empty end nodes should produce empty slice";
}

TEST_F(PDGSlicingTest, ContextSensitiveForwardSlicing) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ContextSensitiveSlicing csSlicer(*PDG);
  auto csSlice = csSlicer.computeForwardSlice(*testNodes[0]);
  validateSlice(csSlice, testNodes[0], "Context-sensitive slice");
}

TEST_F(PDGSlicingTest, ContextSensitiveBackwardSlicing) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ContextSensitiveSlicing csSlicer(*PDG);
  auto csSlice = csSlicer.computeBackwardSlice(*testNodes[0]);
  validateSlice(csSlice, testNodes[0], "Context-sensitive backward slice");
}

TEST_F(PDGSlicingTest, ContextSensitiveChop) {
  if (!hasMinimumNodes(2)) {
    GTEST_SKIP() << "Need at least 2 test nodes for context-sensitive chop";
    return;
  }
  
  ContextSensitiveSlicing csSlicer(*PDG);
  auto csChop = csSlicer.computeChop(*testNodes[0], *testNodes[1]);
  
  EXPECT_GE(csChop.size(), 0) << "Context-sensitive chop should be non-negative size";
  auto stats = SlicingUtils::getSliceStatistics(csChop);
  EXPECT_GE(stats["total_nodes"], 0) << "Chop should have valid statistics";
}

TEST_F(PDGSlicingTest, ContextSensitivePathFinding) {
  if (!hasMinimumNodes(2)) {
    GTEST_SKIP() << "Need at least 2 test nodes for context-sensitive path finding";
    return;
  }
  
  ContextSensitiveSlicing csSlicer(*PDG);
  bool hasPath = csSlicer.hasContextSensitivePath(*testNodes[0], *testNodes[1]);
  
  // Path finding may or may not find a path depending on the test data
  // Just verify the function doesn't crash and returns a boolean
  EXPECT_TRUE(hasPath == true || hasPath == false) << "Should return a valid boolean";
}

TEST_F(PDGSlicingTest, ContextSensitiveEdgeTypeFiltering) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ContextSensitiveSlicing csSlicer(*PDG);
  Node *startNode = testNodes[0];
  
  auto dataSlice = csSlicer.computeForwardSlice(*startNode, SlicingUtils::getDataDependencyEdges());
  auto controlSlice = csSlicer.computeForwardSlice(*startNode, SlicingUtils::getControlDependencyEdges());
  auto callReturnSlice = csSlicer.computeForwardSlice(*startNode, ContextSensitiveSlicingUtils::getCallReturnEdges());
  
  EXPECT_TRUE(dataSlice.find(startNode) != dataSlice.end());
  EXPECT_TRUE(controlSlice.find(startNode) != controlSlice.end());
  EXPECT_TRUE(callReturnSlice.find(startNode) != callReturnSlice.end());
}

TEST_F(PDGSlicingTest, ContextSensitiveVsContextInsensitiveComparison) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  Node *startNode = testNodes[0];
  
  // Compute both slice types
  ForwardSlicing ciSlicer(*PDG);
  auto ciSlice = ciSlicer.computeSlice(*startNode);
  
  ContextSensitiveSlicing csSlicer(*PDG);
  auto csSlice = csSlicer.computeForwardSlice(*startNode);
  
  // Compare slices
  auto comparison = ContextSensitiveSlicingUtils::compareSlices(csSlice, ciSlice);
  
  EXPECT_GT(comparison["cs_slice_size"], 0) << "Context-sensitive slice should not be empty";
  EXPECT_GT(comparison["ci_slice_size"], 0) << "Context-insensitive slice should not be empty";
  
  // Context-sensitive slice should be at most as large as context-insensitive slice
  EXPECT_LE(comparison["cs_slice_size"], comparison["ci_slice_size"]) 
    << "Context-sensitive slice should be smaller or equal to context-insensitive slice";
  
  // Common nodes should exist
  EXPECT_GT(comparison["common_nodes"], 0) 
    << "There should be some common nodes between both slices";
}

TEST_F(PDGSlicingTest, ContextSensitiveUtilities) {
  if (!PDG) {
    GTEST_SKIP() << "No PDG available";
    return;
  }
  
  // Test call/return edge utilities
  auto callReturnEdges = ContextSensitiveSlicingUtils::getCallReturnEdges();
  EXPECT_FALSE(callReturnEdges.empty()) << "Call/return edges should not be empty";
  
  // Verify specific edge types are included
  const std::vector<EdgeType> expectedEdges = {
    EdgeType::CONTROLDEP_CALLINV, EdgeType::CONTROLDEP_CALLRET,
    EdgeType::PARAMETER_IN, EdgeType::PARAMETER_OUT, EdgeType::DATA_RET
  };
  
  for (auto edgeType : expectedEdges) {
    EXPECT_TRUE(callReturnEdges.find(edgeType) != callReturnEdges.end());
  }
}

TEST_F(PDGSlicingTest, CFLReachabilityStatistics) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  ContextSensitiveSlicing csSlicer(*PDG);
  auto csSlice = csSlicer.computeForwardSlice(*testNodes[0]);
  
  // Get CFL-reachability specific statistics
  auto cflStats = ContextSensitiveSlicingUtils::getCFLReachabilityStatistics(csSlice);
  
  EXPECT_GT(cflStats["total_nodes"], 0) << "Should have nodes in slice";
  EXPECT_TRUE(cflStats.find("call_nodes") != cflStats.end()) << "Should have call node statistics";
  EXPECT_TRUE(cflStats.find("return_nodes") != cflStats.end()) << "Should have return node statistics";
  EXPECT_TRUE(cflStats.find("matched_call_return_pairs") != cflStats.end()) << "Should have matched pair statistics";
}

TEST_F(PDGSlicingTest, CFLValidPath) {
  if (!hasMinimumNodes(2)) {
    GTEST_SKIP() << "Need at least 2 test nodes for CFL path validation";
    return;
  }
  
  // Create a simple path for testing
  std::vector<Node *> testPath = {testNodes[0], testNodes[1]};
  
  // Test CFL path validation
  bool isValid = ContextSensitiveSlicingUtils::isCFLValidPath(testPath, *PDG);
  
  // The result depends on the actual path structure, but the function should not crash
  EXPECT_TRUE(isValid == true || isValid == false) << "Should return a valid boolean";
}

TEST_F(PDGSlicingTest, ContextSensitiveVsCFLComparison) {
  if (!hasMinimumNodes(1)) {
    GTEST_SKIP() << "No PDG or test nodes available";
    return;
  }
  
  Node *startNode = testNodes[0];
  
  // Compute both slice types
  ForwardSlicing ciSlicer(*PDG);
  auto ciSlice = ciSlicer.computeSlice(*startNode);
  
  ContextSensitiveSlicing csSlicer(*PDG);
  auto csSlice = csSlicer.computeForwardSlice(*startNode);
  
  // Get detailed statistics
  auto cflStats = ContextSensitiveSlicingUtils::getCFLReachabilityStatistics(csSlice);
  auto csStats = ContextSensitiveSlicingUtils::getContextSensitiveSliceStatistics(csSlice);
  auto ciStats = SlicingUtils::getSliceStatistics(ciSlice);
  
  // Context-sensitive slice should be at most as large as context-insensitive slice
  EXPECT_LE(csStats["total_nodes"], ciStats["total_nodes"]) 
    << "Context-sensitive slice should be smaller or equal to context-insensitive slice";
  
  // CFL statistics should provide meaningful information
  EXPECT_TRUE(cflStats["total_nodes"] == csStats["total_nodes"]) 
    << "CFL statistics should match context-sensitive slice size";
  
  // If there are calls, we should have call statistics
  if (cflStats["call_nodes"] > 0) {
    EXPECT_GT(cflStats["call_nodes"], 0) << "Should have call nodes when present";
    EXPECT_TRUE(cflStats.find("call_match_percentage") != cflStats.end()) 
      << "Should have call match percentage";
  }
}

int main(int argc, char **argv) {
  ::testing::InitGoogleTest(&argc, argv);
  cl::ParseCommandLineOptions(argc, argv, "PDG Slicing Test\n");
  return RUN_ALL_TESTS();
}
